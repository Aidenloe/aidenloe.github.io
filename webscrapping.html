<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Aiden" />


<title>Web Scrapping</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Aiden Loe</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="about.html">About</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    R 4 Social Scientists
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">What is R?</li>
    <li>
      <a href="introToR.html">Intro to R</a>
    </li>
    <li>
      <a href="dataVisual.html">Data Visualisation</a>
    </li>
    <li>
      <a href="datawrangling.html">Data Wrangling</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Big Data 4 Social Scientists
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">What is ML?</li>
    <li>
      <a href="introToML.html">Intro to ML</a>
    </li>
    <li>
      <a href="webscrapping.html">Web Scrapping</a>
    </li>
    <li>
      <a href="datamining.html">Data Mining</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    IRT 4 Social Scientists
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">What is IRT?</li>
    <li>
      <a href="introToIRT.html">Intro to IRT</a>
    </li>
    <li>
      <a href="irtplots.html">Plotting to IRT</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    CAT 4 Social Scientists
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">What is CAT?</li>
    <li>
      <a href="introToCAT.html">Intro to CAT</a>
    </li>
    <li>
      <a href="catSimulation.html">CAT simulation</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Web Scrapping</h1>
<h4 class="author"><em>Aiden</em></h4>
<h4 class="date"><em>13 June, 2017</em></h4>

</div>


<div id="web-scrapping-part-1" class="section level1">
<h1><strong>Web Scrapping (Part 1)</strong></h1>
<p>Clearly, there are legal and ethical implications when conducting web scrapping.</p>
<p>Here, I am merely showing you how it is done based on open content.</p>
<p>There are many things you can do with web scrapping though. Check out the <a href="https://en.wikipedia.org/wiki/Web_scraping">wiki</a> on it.</p>
<p>Here are some <a href="https://www.quora.com/What-are-examples-of-how-real-businesses-use-web-scraping">examples</a> of companies.</p>
<p><em>A note of caution is that, if you want to scrape data, make sure that you are doing it</em> <strong>legally</strong>.</p>
<div id="scrape-single-web-pages" class="section level2">
<h2><strong>Scrape Single web pages</strong></h2>
<p>We are using wikipedia as an example for web scrapping.</p>
<p>We want to find out how many people are viewing a specific topic for a pariticular month.</p>
<p>The code below will scrape viewership for a specific month of our choice.</p>
<p>In this example, we will check out how popular the topic ‘web_scraping’ is on wikipedia.</p>
<pre class="r"><code>################# Scrapping viewership on one webpage ################
var=201402
url=paste0(&quot;http://stats.grok.se/json/en/&quot;,var,&quot;/web_scraping&quot;)
raw.data &lt;- readLines(url, warn=&quot;F&quot;)  # igore warning, data is scrapped

################### read json data in R ##############
#### Install the rjson package if you don&#39;t have it already
require(rjson) # load rjson

rd &lt;- fromJSON(raw.data) # convert to a list object in R
rd.views &lt;- rd$daily_views # use only the daily_views
rd.views &lt;- unlist(rd.views) # flatten the list
df &lt;- as.data.frame(rd.views) # save into df

df$date &lt;-  as.Date(rownames(df)) # save as dates
colnames(df) &lt;- c(&quot;views&quot;,&quot;date&quot;) # create colnames
rownames(df) &lt;- 1:nrow(df) # create row names as index</code></pre>
</div>
<div id="plot-viewership-by-a-month" class="section level2">
<h2><strong>Plot viewership by a month</strong></h2>
<p>Once you the data in a neat format. We can start plotting it.</p>
<p>Here you want to visualise the number of viewerships using ggplot2.</p>
<p>We observe a sudden spike somewhere in the middle of feb 2014. I wonder why…</p>
<pre class="r"><code>#### Install these packages if you don&#39;t have them
require(ggplot2) 
require(lubridate)

ggplot(df,aes(x =date, y =views))+   # data frame as first argument, followed by the aesthetics, x and y variables.  
  geom_line()+   # visulisation
  geom_smooth(method=&quot;loess&quot;)+ # smoothing function (try changing the argument to &#39;lm&#39; or &#39;gam&#39;)
  theme_bw(base_size=10) # sets the thumb size to be it legible. </code></pre>
<p><img src="webscrapping_files/figure-html/plot%20data-1.png" width="672" /></p>
</div>
<div id="scrape-data-by-months" class="section level2">
<h2><strong>Scrape data by months</strong></h2>
<p>Sometimes it is more fun to find out viewership based on a certain number of months.</p>
<p>Hence, using ifelse statements and for loop, one can easily scrape the viewership based on the number of months.</p>
<p>Because the weblink for the months &lt; 10 has a 0 digit before the values (i.e., 01, 02, 03…), we need to think of a way to add a 0 before the single digit. In this case we created an ifelse statement to help us with that. Again, this is just one way of doing it. Another way is to first loop through 0-9, then another loop from 10-12. Finally, you can just cbind them together and remove NAs.</p>
<pre class="r"><code>## Looping through 12 months
date =NULL 
dates=NULL 
  for (month in 1:12){
    if(month &lt; 10){
      date[month] = paste0(2014,0,month) #add a 0 before the numeric value
    }else{
      dates[month] = paste0(2014,month)
    }
  }
# Store the data in &#39;all.dates&#39; variable ##
  dates &lt;- na.omit(dates) # remove NAs
  all.dates &lt;- c(date, dates) # concatenate them together
  all.dates &lt;- as.numeric(all.dates) # convert data class to numeric</code></pre>
</div>
<div id="scrape-data-by-years" class="section level2">
<h2><strong>Scrape data by years</strong></h2>
<p>I am sure you can start seeing a pattern now.</p>
<p>By introducing a double for loop, the years could also be extracted out as well.</p>
<p>If you want, you can also extend this to create a triple for loop. The final outer loop could be the language for example.</p>
<pre class="r"><code>## Looping through XXX Years and 12 months 
date =NULL 
dates=NULL 
years &lt;- NULL
for (year in 2012:2014){
  for (month in 1:12){
    if(month &lt; 10){
    date[month] = paste0(year,0,month)
    }else{
    dates[month] = paste0(year,month)
    }
  }
  ####### Store the data in &#39;all.dates&#39; variable ###
  dates &lt;- na.omit(dates) # remove NAs
  all.dates &lt;- c(date, dates) # concatenate them together
  all.dates &lt;- as.numeric(all.dates) # convert data class to numeric
  years &lt;- c(all.dates,years) #combind months according to years
}</code></pre>
</div>
<div id="scrape-data-by-years-url" class="section level2">
<h2><strong>Scrape data by years (URL)</strong></h2>
<p>Based on what we learned above, we can easily swap it to include the relevant links.</p>
<p>Here we will scrap data based on the topic ‘web_scrapping’ between 2013 and 2014.</p>
<pre class="r"><code>## Looping through Years and 12 months based on URL
topic &lt;- &quot;web_scraping&quot;
start.year &lt;- 2013
end.year &lt;- 2014

### fetch data ###
date =NULL 
dates=NULL 
years &lt;- NULL
for (year in start.year:end.year){
  for (month in 1:12){
    if(month &lt; 10){
      date[month] = paste0(&quot;http://stats.grok.se/json/en/&quot;,year,0,month,&quot;/&quot;,topic,&quot;&quot;) #add a zero before the numeric value
    }else{
      dates[month] =paste0(&quot;http://stats.grok.se/json/en/&quot;,year,month,&quot;/&quot;,topic,&quot;&quot;)
    }
  }
  ####### Store the data in &#39;years&#39; variable ###
  dates &lt;- na.omit(dates) # remove NAs
  all.dates &lt;- c(date, dates) # concatenate months
  years &lt;- c(all.dates,years) # concatenate years
}

raw.data &lt;- NULL
for(i in 1:length(years)){
raw.data[i] &lt;- readLines(years[i], warn=&quot;F&quot;)  # igore warning
}

#### Organise Data ###
rd &lt;- NULL
rd.views &lt;- NULL
for(i in 1:length(raw.data)){
rd[[i]] &lt;- fromJSON(raw.data[i]) # convert to a list object in R
rd.views[[i]]&lt;- rd[[i]]$daily_views
}

rd.views.2 &lt;- unlist(rd.views)
df &lt;- as.data.frame(rd.views.2)
df$date &lt;- as.Date(names(rd.views.2)) # save as dates
colnames(df) &lt;- c(&quot;views&quot;,&quot;date&quot;) # create colnames
rownames(df) &lt;- 1:nrow(df) # create row names as index
df &lt;- na.omit(df) #remove NAs. </code></pre>
</div>
<div id="plot-viewership-by-years" class="section level2">
<h2><strong>Plot viewership by years</strong></h2>
<p>Once the data is organised properly, we can plot it.</p>
<pre class="r"><code>ggplot(df,aes(x =date, y =views))+   # data frame as first argument, followed by the aesthetics, x and y variables.  
  geom_line()+   # visulisation
  geom_smooth(method=&quot;loess&quot;)+ # smoothing function (try changing the argument to &#39;lm&#39; or &#39;gam&#39;)
  theme_bw(base_size=10)+ # sets the thumb size to be it legible. 
  labs(title=topic)</code></pre>
<p><img src="webscrapping_files/figure-html/Plot%20URL%20data-1.png" width="672" /></p>
</div>
</div>
<div id="web-scrapping-part-2" class="section level1">
<h1><strong>Web Scrapping (Part 2)</strong></h1>
<p>Scrapping viewership is one area of web scrapping, but perhaps you might be interested in doing sentiment analysis on content.</p>
<p>So we want to extract the contents of the web pages rather than number of times someone viewed the web page.</p>
<div id="scrape-content-wiki" class="section level2">
<h2><strong>Scrape content (Wiki)</strong></h2>
<p>We will be using the <code>RCurl</code> and <code>XML</code> package to help us with the scrapping.</p>
<p>Let’s use the Eurovision_Song_Contest as an example.</p>
<p>The <code>XML</code> package has plenty functions that can allow us to scrape the data.</p>
<p>Usually we are extracting information based on the tags of the web pages.</p>
<pre class="r"><code>##### SCRAPPING CONTENT OFF WEBSITES ######
require(RCurl)
require(XML)
# XPath is a language for querying XML 
# //Select anywhere in the document
# /Select from root
# @select attributes. Used in [] brackets

#### Wikipedia Example ####
url &lt;- &quot;https://en.wikipedia.org/wiki/Eurovision_Song_Contest&quot;
txt = getURL(url) # get the URL html code

# parsing html code into readable format
PARSED &lt;- htmlParse(txt)

# Parsing code using tags
xpathSApply(PARSED, &quot;//h1&quot;)

# strops code and return content of the tag
xpathSApply(PARSED, &quot;//h1&quot;, xmlValue) # h1 tag
xpathSApply(PARSED, &quot;//h3&quot;, xmlValue) # h3 tag
xpathSApply(PARSED, &quot;//a[@href]&quot;) # a tag with href attribute


# Go to url 
# Highlight references
# right click, inspect element
# Search for tags
xpathSApply(PARSED, &quot;//span[@class=&#39;reference-text&#39;]&quot;,xmlValue) # parse notes and citations
xpathSApply(PARSED, &quot;//cite[@class=&#39;citation news&#39;]&quot;,xmlValue) # parse citation news
xpathSApply(PARSED, &quot;//span[@class=&#39;mw-headline&#39;]&quot;,xmlValue) # parse headlines
xpathSApply(PARSED, &quot;//p&quot;,xmlValue) # parsing contents in p tag
xpathSApply(PARSED, &quot;//cite[@class=&#39;citation news&#39;]/a/@href&quot;) # parse links under citation. xmlValue not needed. 
xpathSApply(PARSED, &quot;//p/a/@href&quot;) # parse href links under all p tags
xpathSApply(PARSED, &quot;//p/a/@*&quot;) # parse all atributes under all p tags

# Partial matches - subtle variations within or between pages. 
xpathSApply(PARSED, &quot;//cite[starts-with(@class, &#39;citation news&#39;)]&quot;,xmlValue) # parse citataion news that starts with..
xpathSApply(PARSED, &quot;//cite[contains(@class, &#39;citation news&#39;)]&quot;,xmlValue) # parse citataion news that contains.

# Parsing tree like structure
parsed&lt;-   htmlTreeParse(txt, asText = TRUE)</code></pre>
</div>
<div id="scrape-content-bbc" class="section level2">
<h2><strong>Scrape content (BBC)</strong></h2>
<p>When you know the structure of the data.</p>
<p>All you need to do is to find the correct function to scrape.</p>
<pre class="r"><code>##### BBC Example ####
url &lt;- &quot;http://www.bbc.co.uk/news/uk-england-london-40196565&quot;
url &lt;- &quot;http://www.bbc.co.uk/news/world-asia-40188215&quot;
txt = getURL(url) # get the URL html code

# parsing html code into readable format
PARSED &lt;- htmlParse(txt)
xpathSApply(PARSED, &quot;//h1&quot;, xmlValue) # h1 tag
xpathSApply(PARSED, &quot;//p&quot;, xmlValue) # p tag
xpathSApply(PARSED, &quot;//p[@class=&#39;story-body__introduction&#39;]&quot;, xmlValue) # p tag body
xpathSApply(PARSED, &quot;//div[@class=&#39;date date--v2&#39;]&quot;,xmlValue) # date, only the first is enough
xpathSApply(PARSED, &quot;//meta[@name=&#39;OriginalPublicationDate&#39;]/@content&quot;) # sometimes there is meta data. </code></pre>
</div>
<div id="create-simple-bbc-scrapper" class="section level2">
<h2><strong>Create simple BBC scrapper</strong></h2>
<p>Sometimes, creating a function will make your life better and make your script look simpler.</p>
<pre class="r"><code>##### Create simple BBC scrapper #####
# scrape title, date and content
BBCscrapper1&lt;- function(url){
  txt = getURL(url) # get the URL html code
  PARSED &lt;- htmlParse(txt) # Parse code into readable format
  title &lt;- xpathSApply(PARSED, &quot;//h1&quot;, xmlValue) # h1 tag
  paragraph &lt;- xpathSApply(PARSED, &quot;//p&quot;, xmlValue) # p tag
  date &lt;- xpathSApply(PARSED, &quot;//div[@class=&#39;date date--v2&#39;]&quot;,xmlValue) # date, only the first is enough
  date &lt;- date[1]
  return(cbind(title,date))
  #return(as.matrix(c(title,date)))
}


# Use function that was just created. 
BBCscrapper1(&quot;http://www.bbc.co.uk/news/world-asia-40188215&quot;)</code></pre>
<pre><code>##      title                                                   date         
## [1,] &quot;Japanese fugitive on the run for 45 years is arrested&quot; &quot;7 June 2017&quot;</code></pre>
</div>
<div id="keeping-it-neat" class="section level2">
<h2><strong>Keeping it neat</strong></h2>
<p>Using the <code>plyr</code> package helps to arrange the data in an organised way.</p>
<pre class="r"><code>## Putting the title and date into a dataframe
require(plyr)
#url
url&lt;- c(&quot;http://www.bbc.co.uk/news/world-asia-40188215&quot;, &quot;http://www.bbc.co.uk/news/world-asia-40171379&quot;)
## ldply: For each element of a list, apply function then combine results into a data frame
#put into a dataframe
ldply(url,BBCscrapper1)</code></pre>
<pre><code>##                                                        title        date
## 1      Japanese fugitive on the run for 45 years is arrested 7 June 2017
## 2 Taliban territory: Life in Afghanistan under the militants 8 June 2017</code></pre>
</div>
</div>
<div id="web-scrapping-part-3" class="section level1">
<h1><strong>Web Scrapping (Part 3)</strong></h1>
<p>This example below is taken from code kindly written by David stillwell.</p>
<p>Some editing has been made to the original code.</p>
<div id="scrape-from-wiki-tables" class="section level2">
<h2><strong>Scrape from Wiki tables</strong></h2>
<p>You have learned how to scrape viewership on wikipedia and content on web pages.</p>
<p>This section is about scrapping data tables online.</p>
<pre class="r"><code># Install the packages that you don&#39;t have first. 
library(&quot;RCurl&quot;) # Good package for getting things from URLs, including https
library(&quot;XML&quot;) # Has a good function for parsing HTML data
library(&quot;rvest&quot;) #another package that is good for web scraping. We use it in the Wikipedia example

#####################
### Get a table of data from Wikipedia
## all of this happens because of the read_html function in the rvest package
# First, grab the page source
us_states = read_html(&quot;https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population&quot;) %&gt;% # piping
  # then extract the first node with class of wikitable
  html_node(&quot;.wikitable&quot;) %&gt;% 
  # then convert the HTML table into a data frame
  html_table()</code></pre>
</div>
<div id="scrape-from-online-tables" class="section level2">
<h2><strong>Scrape from online tables</strong></h2>
<p>If we can have two data tables that have at least one column with the same name, then we can merge them together.</p>
<p>The main idea is to link the data together to run simple analysis.</p>
<p>In this case we can get data about <a href="http://apps.saferoutesinfo.org/legislation_funding/state_apportionment.cfm">funding</a> given to various US states to support building infrastructure to improve students’ ability to walk and bike to school.</p>
<pre class="r"><code>######################
url &lt;- &quot;http://apps.saferoutesinfo.org/legislation_funding/state_apportionment.cfm&quot;
funding&lt;-htmlParse(url) #get the data
# find the table on the page and read it into a list object
funding&lt;- readHTMLTable(funding,stringsAsFactors = FALSE)
funding.df &lt;- do.call(&quot;rbind&quot;, funding) #flatten data
# Contain empty spaces previously.
colnames(funding.df)[1]&lt;- c(&quot;State&quot;) # shorten colname to just State. 

# Match up the tables by State/Territory names
# so we have two data frames, x and y, and we&#39;re setting the columns we want to do the matching on by setting by.x and by.y
mydata = merge(us_states, funding.df, by.x=&quot;State or territory&quot;, by.y=&quot;State&quot;)
# it looks pretty good, but note that we&#39;re down to 50 US States, because the others didn&#39;t match up by name
# e.g. &quot;District of Columbia&quot; in the us_states data, doesn&#39;t match &quot;Dist. of Col.&quot; in the funding data

#Replace the total spend column name with a name that&#39;s easier to use.
colnames(mydata)[18] = &quot;total_spend&quot;

#  We need to remove commas so that R can treat it as a number.
mydata[,&quot;Population estimate, July 1, 2016[4]&quot;] = gsub(&quot;,&quot;, &quot;&quot;, mydata[,&quot;Population estimate, July 1, 2016[4]&quot;]) 
mydata[,&quot;Population estimate, July 1, 2016[4]&quot;] = as.numeric(mydata[,&quot;Population estimate, July 1, 2016[4]&quot;]) #this converts it to a number data type

# Now we have to do the same thing with the funding totals, which are in a format like this: $17,309,568
mydata[,&quot;total_spend&quot;] = gsub(&quot;,&quot;, &quot;&quot;, mydata[,&quot;total_spend&quot;]) #this removes all commas
mydata[,&quot;total_spend&quot;] = gsub(&quot;\\$&quot;, &quot;&quot;, mydata[,&quot;total_spend&quot;]) #this removes all dollar signs. We have a \\ because the dollar sign is a special character.
mydata[,&quot;total_spend&quot;] = as.numeric(mydata[,&quot;total_spend&quot;]) #this converts it to a number data type

# Now we can do the plotting
options(scipen=9999) #stop it showing scientific notation
plot(mydata[,&quot;Population estimate, July 1, 2016[4]&quot;], mydata[,&quot;total_spend&quot;])</code></pre>
<p><img src="webscrapping_files/figure-html/Scrape%20Table%20(non-wiki)-1.png" width="672" /></p>
<pre class="r"><code>## What&#39;s does the correlation between state funding and state population look like?
cor(mydata[,&quot;Population estimate, July 1, 2016[4]&quot;], mydata[,&quot;total_spend&quot;]) # 0.9924265 - big correlation!</code></pre>
<pre><code>## [1] 0.9924265</code></pre>
</div>
<div id="plot-funding-data-on-map" class="section level2">
<h2><strong>Plot funding data on map</strong></h2>
<p>Perhaps it might be more interesting to see how the data is like on a map.</p>
<p>We can utilise <code>map_data</code> function in the <code>ggplot</code> package to help us with that.</p>
<p>Again, with a bit of data manipulation, we can merge the data table that contains the longitude and latitude information together with the funding data across different states.</p>
<pre class="r"><code>require(ggplot2)
all_states &lt;- map_data(&quot;state&quot;) # states
colnames(mydata)[1] &lt;- &quot;state&quot; # rename to states
mydata$state &lt;- tolower(mydata$state) #set all to lower case
Total &lt;- merge(all_states, mydata, by.x=&quot;region&quot;, by.y = &#39;state&#39;) # merge data
# we have data for delaware but not lat, long data in the maps
i &lt;- which(!unique(all_states$region) %in% mydata$state) 

# Plot data
ggplot() + 
  geom_polygon(data=Total, aes(x=long, y=lat, group = group, fill=Total$total_spend),colour=&quot;white&quot;) + 
  scale_fill_continuous(low = &quot;thistle2&quot;, high = &quot;darkred&quot;, guide=&quot;colorbar&quot;) + 
  theme_bw()  + 
  labs(fill = &quot;Funding for School&quot; ,title = &quot;Funding for School between 2005 to 2012&quot;, x=&quot;&quot;, y=&quot;&quot;) + 
  scale_y_continuous(breaks=c()) +
  scale_x_continuous(breaks=c()) +
  theme(panel.border =  element_blank(),
        text = element_text(size=20))</code></pre>
<p><img src="webscrapping_files/figure-html/Plot%20on%20Map-1.png" width="1728" /></p>
</div>
</div>
<div id="web-scrapping-part-4" class="section level1">
<h1><strong>Web Scrapping (Part 4)</strong></h1>
<div id="scrape-from-discussion-forums" class="section level2">
<h2><strong>Scrape from discussion forums</strong></h2>
<p>Coming soon..</p>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
